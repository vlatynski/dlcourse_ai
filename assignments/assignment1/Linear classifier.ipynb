{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sqr' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-df9423122bb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcheck_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0marray_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sqr' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b967ef367044>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TODO Implement softmax and cross-entropy for single sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/github/dlcourse_ai/assignments/assignment1/linear_classifer.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(predictions)\u001b[0m\n\u001b[1;32m     15\u001b[0m     '''\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# TODO implement softmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0msoft_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#print(\"SOFTMAX\", soft_max)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   2318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2319\u001b[0m     return _methods._amax(a, axis=axis,\n\u001b[0;32m-> 2320\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   2321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_amax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# small reductions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_amax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_amin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "\n",
    "print(probs)\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "print(probs)\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-4272065c75a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_classifer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/github/dlcourse_ai/assignments/assignment1/linear_classifer.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(predictions)\u001b[0m\n\u001b[1;32m     15\u001b[0m     '''\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# TODO implement softmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0msoft_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#print(\"SOFTMAX\", soft_max)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   2318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2319\u001b[0m     return _methods._amax(a, axis=axis,\n\u001b[0;32m-> 2320\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   2321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_amax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# small reductions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_amax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_amin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "\n",
    "\n",
    "print(linear_classifer.cross_entropy_loss(probs, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-903c2f1cab64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#print(\"GRAD:\",grad)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#print(\"*\"*80)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcheck_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_with_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/github/dlcourse_ai/assignments/assignment1/gradient_check.py\u001b[0m in \u001b[0;36mcheck_gradient\u001b[0;34m(f, x, delta, tol)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0morig_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mfx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalytic_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Functions shouldn't modify input variables\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalytic_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-903c2f1cab64>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#print(\"GRAD:\",grad)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#print(\"*\"*80)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcheck_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_with_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/github/dlcourse_ai/assignments/assignment1/linear_classifer.py\u001b[0m in \u001b[0;36msoftmax_with_cross_entropy\u001b[0;34m(predictions, target_index)\u001b[0m\n\u001b[1;32m     94\u001b[0m     '''\n\u001b[1;32m     95\u001b[0m     \u001b[0;31m# TODO implement softmax with cross-entropy minibatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0msft_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0mreal1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;31m#print(\"REAL1 BEFORE \",real1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/github/dlcourse_ai/assignments/assignment1/linear_classifer.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(predictions)\u001b[0m\n\u001b[1;32m     15\u001b[0m     '''\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# TODO implement softmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0msoft_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#print(\"SOFTMAX\", soft_max)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   2318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2319\u001b[0m     return _methods._amax(a, axis=axis,\n\u001b[0;32m-> 2320\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   2321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_amax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# small reductions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_amax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_amin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "#loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "#print(\"LOSS:\",loss)\n",
    "#print(\"GRAD:\",grad)\n",
    "#print(\"*\"*80)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2. -1.  1.]]\n",
      "REAL1 BEFORE  [[0. 0. 0. 0.]]\n",
      "TARGET [[2]]\n",
      "(1, 4) (1, 4)\n",
      "analytic grads=  [[ 0.20603191  0.56005279 -0.97211661  0.20603191]]\n",
      "X= [[ 1.  2. -1.  1.]]\n",
      "\u001b[91mix=  (0, 0) \u001b[0m\n",
      "analgrad_at_ix=  0.20603190919001857\n",
      "X - [[ 0.99999  2.      -1.       1.     ]]\n",
      "X + [[ 1.00001  2.      -1.       1.     ]]\n",
      "REAL1 BEFORE  [[0. 0. 0. 0.]]\n",
      "TARGET [[2]]\n",
      "REAL1 BEFORE  [[0. 0. 0. 0.]]\n",
      "TARGET [[2]]\n",
      "\u001b[91mcomparing numeric grad (calculated)  0.20603190920009948  with analytical  0.20603190919001857 \u001b[0m\n",
      "\u001b[91mix=  (0, 1) \u001b[0m\n",
      "analgrad_at_ix=  0.5600527948339517\n",
      "X - [[ 1.       1.99999 -1.       1.     ]]\n",
      "X + [[ 1.       2.00001 -1.       1.     ]]\n",
      "REAL1 BEFORE  [[0. 0. 0. 0.]]\n",
      "TARGET [[2]]\n",
      "REAL1 BEFORE  [[0. 0. 0. 0.]]\n",
      "TARGET [[2]]\n",
      "\u001b[91mcomparing numeric grad (calculated)  0.560052794829069  with analytical  0.5600527948339517 \u001b[0m\n",
      "\u001b[91mix=  (0, 2) \u001b[0m\n",
      "analgrad_at_ix=  -0.9721166132139888\n",
      "X - [[ 1.       2.      -1.00001  1.     ]]\n",
      "X + [[ 1.       2.      -0.99999  1.     ]]\n",
      "REAL1 BEFORE  [[0. 0. 0. 0.]]\n",
      "TARGET [[2]]\n",
      "REAL1 BEFORE  [[0. 0. 0. 0.]]\n",
      "TARGET [[2]]\n",
      "\u001b[91mcomparing numeric grad (calculated)  -0.9721166132292679  with analytical  -0.9721166132139888 \u001b[0m\n",
      "\u001b[91mix=  (0, 3) \u001b[0m\n",
      "analgrad_at_ix=  0.20603190919001857\n",
      "X - [[ 1.       2.      -1.       0.99999]]\n",
      "X + [[ 1.       2.      -1.       1.00001]]\n",
      "REAL1 BEFORE  [[0. 0. 0. 0.]]\n",
      "TARGET [[2]]\n",
      "REAL1 BEFORE  [[0. 0. 0. 0.]]\n",
      "TARGET [[2]]\n",
      "\u001b[91mcomparing numeric grad (calculated)  0.20603190920009948  with analytical  0.20603190919001857 \u001b[0m\n",
      "\u001b[42mGradient check passed!\u001b[0m\n",
      "************************************************************\n",
      "REAL1 BEFORE  [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "TARGET [[3]\n",
      " [3]\n",
      " [2]]\n",
      "(3, 4) (3, 4)\n",
      "analytic grads=  [[ 0.1218431   0.04482357 -0.28850976 -0.21149024]\n",
      " [ 0.02753151  0.07483841 -0.12990144 -0.30580182]\n",
      " [ 0.00929446  0.0686773  -0.26465603 -0.14664907]]\n",
      "X= [[ 2.  1.  1.  2.]\n",
      " [-1.  0.  1. -1.]\n",
      " [-1.  1.  1.  2.]]\n",
      "\u001b[91mix=  (0, 0) \u001b[0m\n",
      "analgrad_at_ix=  0.12184309643833414\n",
      "X - [[ 1.99999  1.       1.       2.     ]\n",
      " [-1.       0.       1.      -1.     ]\n",
      " [-1.       1.       1.       2.     ]]\n",
      "X + [[ 2.00001  1.       1.       2.     ]\n",
      " [-1.       0.       1.      -1.     ]\n",
      " [-1.       1.       1.       2.     ]]\n",
      "REAL1 BEFORE  [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "TARGET [[3]\n",
      " [3]\n",
      " [2]]\n",
      "REAL1 BEFORE  [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "TARGET [[3]\n",
      " [3]\n",
      " [2]]\n",
      "\u001b[91mcomparing numeric grad (calculated)  0.24368619286896373  with analytical  0.12184309643833414 \u001b[0m\n",
      "\u001b[91m--------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[91mGradients are different at (0, 0). Analytic: 0.12184, Numeric: 0.24369\u001b[0m\n",
      "********************************************************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(num_classes, batch_size)).astype(np.float)\n",
    "print(predictions.T)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions.T)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(num_classes, batch_size)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3) (3, 2)\n",
      "X= [[-1. -1.  1.]\n",
      " [ 0.  1.  1.]]\n",
      "W [[ 1.  2.]\n",
      " [-1.  1.]\n",
      " [ 1.  2.]]\n",
      "(3, 2) (3, 2)\n",
      "analytic grads=  [[-0.44039854  0.44039854]\n",
      " [-0.4166856   0.4166856 ]\n",
      " [ 0.46411148 -0.46411148]]\n",
      "X= [[ 1.  2.]\n",
      " [-1.  1.]\n",
      " [ 1.  2.]]\n",
      "\u001b[91mix=  (0, 0) \u001b[0m\n",
      "analgrad_at_ix=  -0.44039853898894116\n",
      "X - [[ 0.99999  2.     ]\n",
      " [-1.       1.     ]\n",
      " [ 1.       2.     ]]\n",
      "X + [[ 1.00001  2.     ]\n",
      " [-1.       1.     ]\n",
      " [ 1.       2.     ]]\n",
      "\u001b[91mcomparing numeric grad (calculated)  -0.4403985389922482  with analytical  -0.44039853898894116 \u001b[0m\n",
      "\u001b[91mix=  (0, 1) \u001b[0m\n",
      "analgrad_at_ix=  0.4403985389889412\n",
      "X - [[ 1.       1.99999]\n",
      " [-1.       1.     ]\n",
      " [ 1.       2.     ]]\n",
      "X + [[ 1.       2.00001]\n",
      " [-1.       1.     ]\n",
      " [ 1.       2.     ]]\n",
      "\u001b[91mcomparing numeric grad (calculated)  0.4403985389922482  with analytical  0.4403985389889412 \u001b[0m\n",
      "\u001b[91mix=  (1, 0) \u001b[0m\n",
      "analgrad_at_ix=  -0.4166856024001578\n",
      "X - [[ 1.       2.     ]\n",
      " [-1.00001  1.     ]\n",
      " [ 1.       2.     ]]\n",
      "X + [[ 1.       2.     ]\n",
      " [-0.99999  1.     ]\n",
      " [ 1.       2.     ]]\n",
      "\u001b[91mcomparing numeric grad (calculated)  -0.4166856024112597  with analytical  -0.4166856024001578 \u001b[0m\n",
      "\u001b[91mix=  (1, 1) \u001b[0m\n",
      "analgrad_at_ix=  0.4166856024001579\n",
      "X - [[ 1.       2.     ]\n",
      " [-1.       0.99999]\n",
      " [ 1.       2.     ]]\n",
      "X + [[ 1.       2.     ]\n",
      " [-1.       1.00001]\n",
      " [ 1.       2.     ]]\n",
      "\u001b[91mcomparing numeric grad (calculated)  0.4166856024112597  with analytical  0.4166856024001579 \u001b[0m\n",
      "\u001b[91mix=  (2, 0) \u001b[0m\n",
      "analgrad_at_ix=  0.46411147557772453\n",
      "X - [[ 1.       2.     ]\n",
      " [-1.       1.     ]\n",
      " [ 0.99999  2.     ]]\n",
      "X + [[ 1.       2.     ]\n",
      " [-1.       1.     ]\n",
      " [ 1.00001  2.     ]]\n",
      "\u001b[91mcomparing numeric grad (calculated)  0.4641114755732367  with analytical  0.46411147557772453 \u001b[0m\n",
      "\u001b[91mix=  (2, 1) \u001b[0m\n",
      "analgrad_at_ix=  -0.46411147557772453\n",
      "X - [[ 1.       2.     ]\n",
      " [-1.       1.     ]\n",
      " [ 1.       1.99999]]\n",
      "X + [[ 1.       2.     ]\n",
      " [-1.       1.     ]\n",
      " [ 1.       2.00001]]\n",
      "\u001b[91mcomparing numeric grad (calculated)  -0.4641114755732367  with analytical  -0.46411147557772453 \u001b[0m\n",
      "\u001b[42mGradient check passed!\u001b[0m\n",
      "************************************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "print(X.shape, W.shape)\n",
    "print(\"X=\", X)\n",
    "print(\"W\", W)\n",
    "\n",
    "#X = np.hstack([X, np.ones((X.shape[0], 1))])\n",
    "#print(\"X=\", X)\n",
    "#W = np.vstack([W, np.ones((1,W.shape[1]))])\n",
    "#print(\"W\", W)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2) (3, 2)\n",
      "analytic grads=  [[ 0.02  0.04]\n",
      " [-0.02  0.02]\n",
      " [ 0.02  0.04]]\n",
      "X= [[ 1.  2.]\n",
      " [-1.  1.]\n",
      " [ 1.  2.]]\n",
      "\u001b[91mix=  (0, 0) \u001b[0m\n",
      "analgrad_at_ix=  0.02\n",
      "X - [[ 0.99999  2.     ]\n",
      " [-1.       1.     ]\n",
      " [ 1.       2.     ]]\n",
      "X + [[ 1.00001  2.     ]\n",
      " [-1.       1.     ]\n",
      " [ 1.       2.     ]]\n",
      "\u001b[91mcomparing numeric grad (calculated)  0.020000000000575113  with analytical  0.02 \u001b[0m\n",
      "\u001b[91mix=  (0, 1) \u001b[0m\n",
      "analgrad_at_ix=  0.04\n",
      "X - [[ 1.       1.99999]\n",
      " [-1.       1.     ]\n",
      " [ 1.       2.     ]]\n",
      "X + [[ 1.       2.00001]\n",
      " [-1.       1.     ]\n",
      " [ 1.       2.     ]]\n",
      "\u001b[91mcomparing numeric grad (calculated)  0.03999999999976245  with analytical  0.04 \u001b[0m\n",
      "\u001b[91mix=  (1, 0) \u001b[0m\n",
      "analgrad_at_ix=  -0.02\n",
      "X - [[ 1.       2.     ]\n",
      " [-1.00001  1.     ]\n",
      " [ 1.       2.     ]]\n",
      "X + [[ 1.       2.     ]\n",
      " [-0.99999  1.     ]\n",
      " [ 1.       2.     ]]\n",
      "\u001b[91mcomparing numeric grad (calculated)  -0.020000000000575113  with analytical  -0.02 \u001b[0m\n",
      "\u001b[91mix=  (1, 1) \u001b[0m\n",
      "analgrad_at_ix=  0.02\n",
      "X - [[ 1.       2.     ]\n",
      " [-1.       0.99999]\n",
      " [ 1.       2.     ]]\n",
      "X + [[ 1.       2.     ]\n",
      " [-1.       1.00001]\n",
      " [ 1.       2.     ]]\n",
      "\u001b[91mcomparing numeric grad (calculated)  0.020000000000575113  with analytical  0.02 \u001b[0m\n",
      "\u001b[91mix=  (2, 0) \u001b[0m\n",
      "analgrad_at_ix=  0.02\n",
      "X - [[ 1.       2.     ]\n",
      " [-1.       1.     ]\n",
      " [ 0.99999  2.     ]]\n",
      "X + [[ 1.       2.     ]\n",
      " [-1.       1.     ]\n",
      " [ 1.00001  2.     ]]\n",
      "\u001b[91mcomparing numeric grad (calculated)  0.020000000000575113  with analytical  0.02 \u001b[0m\n",
      "\u001b[91mix=  (2, 1) \u001b[0m\n",
      "analgrad_at_ix=  0.04\n",
      "X - [[ 1.       2.     ]\n",
      " [-1.       1.     ]\n",
      " [ 1.       1.99999]]\n",
      "X + [[ 1.       2.     ]\n",
      " [-1.       1.     ]\n",
      " [ 1.       2.00001]]\n",
      "\u001b[91mcomparing numeric grad (calculated)  0.03999999999976245  with analytical  0.04 \u001b[0m\n",
      "\u001b[42mGradient check passed!\u001b[0m\n",
      "************************************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (9000, 3073)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a18c77978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.093\n",
      "X (9000, 3073)\n",
      "Accuracy after training for 100 epochs:  0.092\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "best_classifier = classifier.fit(train_X, train_y, epochs=150, learning_rate=1e-5, batch_size=300, reg=1e1-6)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (9000, 3073)\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = 0.\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "for lr in learning_rates:\n",
    "    for rs in reg_strengths:\n",
    "        classifier1 = classifier.fit(train_X, train_y, epochs=150, learning_rate=lr, batch_size=300, reg=rs)\n",
    "        pred1 = classifier.predict(val_X)\n",
    "        accuracy1 = multiclass_accuracy(pred1, val_y)\n",
    "        #print(\"Accuracy after training for 100 epochs: \", accuracy1)\n",
    "        if accuracy1 > best_val_accuracy:\n",
    "            best_classifier = classifier1\n",
    "            best_val_accuracy = accuracy1\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.088000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
